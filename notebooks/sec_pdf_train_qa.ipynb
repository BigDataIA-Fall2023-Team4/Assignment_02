{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xhEq--8-MB9"
      },
      "source": [
        "# Train a fine-tuning model specialized for Q&A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS84Ytb4HPe5"
      },
      "source": [
        "In this notebook, we will be using a dataset containing sets of content, questions, and answers. Our objective is to expand this dataset by generating adversarial question and context pairs. These adversarial questions will not have been originally generated in the provided context. In such instances, the model's response will be \"Insufficient context for answering the question.\" Furthermore, we will train a discriminator model designed to predict whether a given question can be answered based on the provided context.\n",
        "\n",
        "To make this process even more challenging, we will incorporate difficult adversarial examples. These examples will be derived from either sections of text that are semantically similar or neighboring sections from the same article."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY-7ZlsFHosX"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QeQW9D1-dgk",
        "outputId": "82675008-370a-4930-e66b-65580aaf15b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/77.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "e2TZEj17-LSG"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lMpzzEutnsjx"
      },
      "outputs": [],
      "source": [
        "openai.api_key = <API-KEY>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "X2spM6_9nunE",
        "outputId": "c974f71b-cb31-45bb-f8c7-0ecbad02e2d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   num_tokens                                            content  \\\n",
              "0         993  OMB APPROVAL OMB Number: 3235-0554 Expires:  F...   \n",
              "1         997  This collection of information has been review...   \n",
              "2         986  The exchange consents that service of any civi...   \n",
              "3         900  Exhibit D Describe the manner of operation of ...   \n",
              "\n",
              "                                           questions  \\\n",
              "0  1. What is the purpose of Form 1-N?\\n2. What i...   \n",
              "1  1. What is the name of the Security Futures Pr...   \n",
              "2  1. What is the name of the form being filed?\\n...   \n",
              "3  1. What is the means of access to the System?\\...   \n",
              "\n",
              "                                             answers  \n",
              "0  1. Form 1-N is the form for notice of registra...  \n",
              "1  1. The Security Futures Product Exchange is th...  \n",
              "2  1. The name of the form being filed is Form 1-...  \n",
              "3  1. The means of access to the System is throug...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c909afc-11cb-42e4-837c-9aea23496ed1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_tokens</th>\n",
              "      <th>content</th>\n",
              "      <th>questions</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>993</td>\n",
              "      <td>OMB APPROVAL OMB Number: 3235-0554 Expires:  F...</td>\n",
              "      <td>1. What is the purpose of Form 1-N?\\n2. What i...</td>\n",
              "      <td>1. Form 1-N is the form for notice of registra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>997</td>\n",
              "      <td>This collection of information has been review...</td>\n",
              "      <td>1. What is the name of the Security Futures Pr...</td>\n",
              "      <td>1. The Security Futures Product Exchange is th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>986</td>\n",
              "      <td>The exchange consents that service of any civi...</td>\n",
              "      <td>1. What is the name of the form being filed?\\n...</td>\n",
              "      <td>1. The name of the form being filed is Form 1-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>900</td>\n",
              "      <td>Exhibit D Describe the manner of operation of ...</td>\n",
              "      <td>1. What is the means of access to the System?\\...</td>\n",
              "      <td>1. The means of access to the System is throug...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c909afc-11cb-42e4-837c-9aea23496ed1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6c909afc-11cb-42e4-837c-9aea23496ed1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6c909afc-11cb-42e4-837c-9aea23496ed1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b6100c9c-4489-4af9-bc0e-065dff86f1eb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b6100c9c-4489-4af9-bc0e-065dff86f1eb')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b6100c9c-4489-4af9-bc0e-065dff86f1eb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/DAMG7245/pdf_content_openai_qa.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_RsgNE1HtdD"
      },
      "source": [
        "Split the dataframe into a training and testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8vbrlg4oCQh",
        "outputId": "7d81e7b3-b5b7-4959-d9b3-78fd41300119"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "len(train_df), len(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw-InBVAvXGG"
      },
      "source": [
        "## Create the fine-tuning datasets for Q&A and discriminator models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaqWzQVYH8HF"
      },
      "source": [
        "The process of constructing the fine-tuning dataset is as follows:\n",
        "\n",
        "1. For each set of related question, answer, and context, we generate the following examples:\n",
        "\n",
        "   - Positive example: A valid question, answer, and context combination.\n",
        "   \n",
        "   - Negative examples:\n",
        "     - A randomly selected context paired with the same question.\n",
        "     - Two challenging negative examples, one sourced from the same sec form and the other being the context most similar to the correct one.\n",
        "\n",
        "2. It's important to note that this process may introduce some noise, as occasionally a different context could potentially answer the same question. However, on average, we anticipate that this noise will not significantly impact performance.\n",
        "\n",
        "3. We repeat the same dataset creation process separately for both the discriminator and the Q&A answering model. Additionally, we perform this process independently for the training and testing datasets to ensure that the examples in the training set do not overlap with those in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YYdbv-7coMJY"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def get_random_similar_contexts(question, context, file_id='https://www.sec.gov/files/form1.pdf', search_model='ada', max_rerank=10):\n",
        "    \"\"\"\n",
        "    Find similar contexts to the given context using the search file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        results = openai.Engine(search_model).search(\n",
        "            search_model=search_model,\n",
        "            query=question,\n",
        "            max_rerank=max_rerank,\n",
        "            file=file_id\n",
        "        )\n",
        "        candidates = []\n",
        "        for result in results['data'][:3]:\n",
        "            if result['text'] == context:\n",
        "                continue\n",
        "            candidates.append(result['text'])\n",
        "        random_candidate = random.choice(candidates)\n",
        "        return random_candidate\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return \"\"\n",
        "\n",
        "def create_fine_tuning_dataset(df, discriminator=False, n_negative=1, add_related=False):\n",
        "    \"\"\"\n",
        "    Create a dataset for fine tuning the OpenAI model; either for a discriminator model,\n",
        "    or a model specializing in Q&A, where it says if no relevant context is found.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df: pd.DataFrame\n",
        "        The dataframe containing the question, answer and context pairs\n",
        "    discriminator: bool\n",
        "        Whether to create a dataset for the discriminator\n",
        "    n_negative: int\n",
        "        The number of random negative samples to add (using a random context)\n",
        "    add_related: bool\n",
        "        Whether to add the related contexts to the correct context. These are hard negative examples\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        The dataframe containing the prompts and completions, ready for fine-tuning\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for i, row in df.iterrows():\n",
        "        for q, a in zip((\"1.\" + row.questions).split('\\n'), (\"1.\" + row.answers).split('\\n')):\n",
        "            if len(q) >10 and len(a) >10:\n",
        "                if discriminator:\n",
        "                    rows.append({\"prompt\":f\"{row.content}\\nQuestion: {q[2:].strip()}\\n Related:\", \"completion\":f\" yes\"})\n",
        "                else:\n",
        "                    rows.append({\"prompt\":f\"{row.content}\\nQuestion: {q[2:].strip()}\\nAnswer:\", \"completion\":f\" {a[2:].strip()}\"})\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        for q in (\"1.\" + row.questions).split('\\n'):\n",
        "            if len(q) >10:\n",
        "                for j in range(n_negative + (2 if add_related else 0)):\n",
        "                    random_context = \"\"\n",
        "                    if j == 0 and add_related:\n",
        "                        # add the related contexts based on originating from the same page\n",
        "                        subset = df[(df.content != row.content)]\n",
        "\n",
        "                        if len(subset) < 1:\n",
        "                            continue\n",
        "                        random_context = subset.sample(1).iloc[0].content\n",
        "                    if j == 1 and add_related:\n",
        "                        # add the related contexts based on the most similar contexts according to the search\n",
        "                        random_context = get_random_similar_contexts(q[2:].strip(), row.content, search_model='ada', max_rerank=10)\n",
        "                    else:\n",
        "                        while True:\n",
        "                            # add random context, which isn't the correct context\n",
        "                            random_context = df.sample(1).iloc[0].content\n",
        "                            if random_context != row.content:\n",
        "                                break\n",
        "                    if discriminator:\n",
        "                        rows.append({\"prompt\":f\"{random_context}\\nQuestion: {q[2:].strip()}\\n Related:\", \"completion\":f\" no\"})\n",
        "                    else:\n",
        "                        rows.append({\"prompt\":f\"{random_context}\\nQuestion: {q[2:].strip()}\\nAnswer:\", \"completion\":f\" No appropriate context found to answer the question.\"})\n",
        "\n",
        "    return pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVQTeLCsISeS"
      },
      "source": [
        "We follow the identical procedure to create datasets for both the discriminator and the Q&A answering model. This process is carried out independently for both the training and testing sets to guarantee that instances from the training set do not appear in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-C5TGTCAvjYH"
      },
      "outputs": [],
      "source": [
        "for name, is_disc in [('discriminator', True), ('qa', False)]:\n",
        "    for train_test, dt in [('train', train_df), ('test', test_df)]:\n",
        "        ft = create_fine_tuning_dataset(dt, discriminator=is_disc, n_negative=1, add_related=True)\n",
        "        ft.to_json(f'{name}_{train_test}.jsonl', orient='records', lines=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBz8EWgeyZuu"
      },
      "source": [
        "## Submit the datasets for fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submit the dataset for fintuning on discriminator and qa model"
      ],
      "metadata": {
        "id": "RxHOOR3Fuz6o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zShz6PNKK8dH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = '<API Key>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY6Bv6OUyURy",
        "outputId": "c7cc87d4-6d2f-4901-c5fd-8536563ac7b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\rUpload progress:   0% 0.00/288k [00:00<?, ?it/s]\rUpload progress: 100% 288k/288k [00:00<00:00, 468Mit/s]\n",
            "Uploaded file from /content/discriminator_train.jsonl: file-Pwk5WaINwHr4qcCqIxUkYx9W\n",
            "Upload progress: 100% 288k/288k [00:00<00:00, 508Mit/s]\n",
            "Uploaded file from /content/discriminator_test.jsonl: file-cH0HdgQjXwpaGh7E5mHPwiFB\n",
            "Created fine-tune: ft-aykOCrFSrBAtZxmRFRhbQ3KF\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2023-10-18 05:05:53] Created fine-tune: ft-aykOCrFSrBAtZxmRFRhbQ3KF\n",
            "[2023-10-18 05:06:07] Fine-tune costs $0.10\n",
            "[2023-10-18 05:06:08] Fine-tune enqueued. Queue number: 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!openai api fine_tunes.create -t \"/content/discriminator_train.jsonl\" -v \"/content/discriminator_test.jsonl\" --batch_size 16  --compute_classification_metrics --classification_positive_class \" yes\" --model ada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWkFwnzWyW4X"
      },
      "outputs": [],
      "source": [
        "!openai api fine_tunes.create -t \"olympics-data/qa_train.jsonl\" -v \"olympics-data/qa_test.jsonl\" --batch_size 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZhAnigV51jI"
      },
      "source": [
        "## Using the fine-tuned models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now use the fine-tuned discriminator and the fine-tuned Q&A model."
      ],
      "metadata": {
        "id": "Yfy4GE44uocm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYRI9ESa5zGz",
        "outputId": "8d7fdf3c-9864-4fd2-a90f-c2c02f0b2c8a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<OpenAIObject at 0x7b0598a110d0> JSON: {\n",
              "   \" What\": -1.4873294,\n",
              "   \"\\n\": -1.1760615\n",
              " }]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "ft_discriminator = \"davinci-instruct-beta-v3\"\n",
        "ft_qa = \"davinci-instruct-beta-v3\"\n",
        "\n",
        "def apply_ft_discriminator(context, question, discriminator_model):\n",
        "    \"\"\"\n",
        "    Apply the fine tuned discriminator to a question, to assess whether it can be answered from the context.\n",
        "    \"\"\"\n",
        "    prompt = f\"{context}\\nQuestion: {question}\\n Related:\"\n",
        "    result = openai.Completion.create(model=discriminator_model, prompt=prompt, max_tokens=1, temperature=0, top_p=1, n=1, logprobs=2)\n",
        "    return result['choices'][0]['logprobs']['top_logprobs']\n",
        "\n",
        "apply_ft_discriminator('Form 1-N is the form for notice of registration as a \\\n",
        "national securities exchange for the sole purpose of trading security futures \\\n",
        "products (“Security Futures Product Exchange”) pursuant to Section 6(g) of the \\\n",
        "Securities Exchange Act of 1934 (“Exchange Act”).',\n",
        "                        'What is the Form 1-N for?', ft_discriminator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "hp4Lry7w54bo",
        "outputId": "e2732216-60e4-4bfc-8045-b07b009029bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The Form 1-N is the form for notice of registration as a national securities exchange for the sole purpose of trading security futures products (“Security'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "def apply_ft_qa_answer(context, question, answering_model):\n",
        "    \"\"\"\n",
        "    Apply the fine tuned discriminator to a question\n",
        "    \"\"\"\n",
        "    prompt = f\"{context}\\nQuestion: {question}\\nAnswer:\"\n",
        "    result = openai.Completion.create(model=answering_model, prompt=prompt, max_tokens=30, temperature=0, top_p=1, n=1, stop=['.','\\n'])\n",
        "    return result['choices'][0]['text']\n",
        "\n",
        "apply_ft_qa_answer('Form 1-N is the form for notice of registration as a \\\n",
        "national securities exchange for the sole purpose of trading security futures \\\n",
        "products (“Security Futures Product Exchange”) pursuant to Section 6(g) of the \\\n",
        "Securities Exchange Act of 1934 (“Exchange Act”).',\n",
        "                        'What is the Form 1-N for?', ft_qa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WVv6nQq_qW_d",
        "outputId": "0425b320-0f5a-49e8-90bf-2c56c3a466f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' 8 1/2 X 11 inches'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "apply_ft_qa_answer('If the information called for by any Exhibit is available in\\\n",
        " printed form, the printed material may be filed provided it does not exceed 8 1/2 X 11 inches in size.',\n",
        "                    'What is the maximum size of printed form?', ft_qa)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFddCghXJErC"
      },
      "source": [
        "It is very evident that the model is capable of providing answers when the context aligns correctly.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mDcoBmxzs1wP",
        "outputId": "9eafd3ed-5ae6-4702-e413-7dc9ca6809b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The number of cars produced in the Soviet Union in 1970 is not available'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "apply_ft_qa_answer('If the information called for by any Exhibit is available in\\\n",
        " printed form, the printed material may be filed provided it does not exceed 8 1/2 X 11 inches in size.',\n",
        "                    'How many cars were produced in the Soviet Union in 1970?', ft_qa)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model could identify above that the context available is not relevant to the qustion."
      ],
      "metadata": {
        "id": "i9OKAT15vCnN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "W2q2URgZs7Kb",
        "outputId": "366df503-9aaf-4b88-f9ee-08f83f60aa51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Yes, the designated contact employee on the Execution Page (Page 1) of Form 1-N authorized to receive all relevant information within the Security Futures'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# This function takes an answering model, a discriminator model, context, a question,\n",
        "# and an optional discriminator_logprob_yes_modifier as input.\n",
        "def answer_question_conditionally(answering_model, discriminator_model, context, question, discriminator_logprob_yes_modifier=0):\n",
        "    # Apply the Fine-Tuned Discriminator to assess the context and question.\n",
        "    logprobs = apply_ft_discriminator(context, question, discriminator_model)\n",
        "\n",
        "    # Extract the log probability for a 'yes' response from the discriminator output,\n",
        "    # or set it to a very low value (-100) if not present.\n",
        "    yes_logprob = logprobs[' yes'] if ' yes' in logprobs else -100\n",
        "\n",
        "    # Extract the log probability for a 'no' response from the discriminator output,\n",
        "    # or set it to a very low value (-100) if not present.\n",
        "    no_logprob = logprobs[' no'] if ' no' in logprobs else -100\n",
        "\n",
        "    # Check if the adjusted 'yes' log probability (with the modifier) is less than the 'no' log probability.\n",
        "    if yes_logprob + discriminator_logprob_yes_modifier < no_logprob:\n",
        "        # If 'no' is more probable, return a message indicating no appropriate context was found.\n",
        "        return \" No appropriate context found to answer the question based on the discriminator.\"\n",
        "\n",
        "    # If 'yes' is more probable, proceed to answer the question using the answering model.\n",
        "    return apply_ft_qa_answer(context, question, answering_model)\n",
        "\n",
        "# Call the answer_question_conditionally function with specific models, context, and question.\n",
        "answer_question_conditionally(ft_qa, ft_discriminator,\n",
        "                            \"The individual listed on the Execution Page (Page 1) \\\n",
        "                             of Form 1-N as the contact employee must be authorized \\\n",
        "                             to receive all contact information, communications, and \\\n",
        "                             mailings and is responsible for disseminating such information \\\n",
        "                             within the Security Futures Product Exchange’s organization\",\n",
        "                            \"Could the designated contact employee on the Execution Page \\\n",
        "                            (Page 1) of Form 1-N authorized to receive all \\\n",
        "                            relevant information within the Security Futures Product Exchange's organization?\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}